{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wanted-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import math\n",
    "import regex\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from json_parser import Parser\n",
    "from common import BBT_PEOPLE, BoundingBox\n",
    "from image_processing import ObjectDetector\n",
    "import language_processing as lp\n",
    "import utils\n",
    "from utils import split_data_set\n",
    "import inference as inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "plain-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hold_base.lp') as f:\n",
    "    background_knowledge = f.read().splitlines()\n",
    "\n",
    "parser = Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-strengthening",
   "metadata": {},
   "source": [
    "# With Ground Truth Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "obvious-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_iter_gt(train_set, test_set):\n",
    "    # 3. Test on test set\n",
    "    jacc_score = 0\n",
    "    num_questions = 0\n",
    "    parsing_error = []\n",
    "    not_full_score = []\n",
    "    zero_score = []\n",
    "\n",
    "    for test in test_set:\n",
    "        pred_ans_idx = inf.inference(test, gt_object=True)\n",
    "        if pred_ans_idx == [-1]:\n",
    "            parsing_error.append(test['qid'])\n",
    "            continue\n",
    "\n",
    "        score = inf.get_jaccard_score(pred_ans_idx, test)\n",
    "\n",
    "        if 0 < score < 1:\n",
    "            not_full_score.append(test['qid'])\n",
    "        elif score == 0:\n",
    "            zero_score.append(test['qid'])\n",
    "\n",
    "        jacc_score += score\n",
    "        num_questions += 1\n",
    "\n",
    "    ##########################################################\n",
    "    \n",
    "    # 4. Print out result\n",
    "    print(F'# tests:     {num_questions}')\n",
    "    print(F'jacc score:   {jacc_score}')\n",
    "    print(F'norm jacc score: {jacc_score / num_questions}')\n",
    "    print()\n",
    "\n",
    "    print('PARSING ERROR')\n",
    "    print(parsing_error)\n",
    "    print()\n",
    "\n",
    "    print('NOT FULL SCORE')\n",
    "    print(not_full_score)\n",
    "    print()\n",
    "\n",
    "    print('ZERO SCORE')\n",
    "    print(zero_score)\n",
    "    \n",
    "    return jacc_score, num_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authentic-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total tests: 886\n"
     ]
    }
   ],
   "source": [
    "with open('hold_questions.json') as f:\n",
    "    attempted_question = json.load(f)\n",
    "\n",
    "print(F'# total tests: {len(attempted_question)}')\n",
    "\n",
    "a_q_dict = dict()\n",
    "\n",
    "for q in attempted_question:\n",
    "    qid = q['qid']\n",
    "    a_q_dict[qid] = q\n",
    "    \n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(attempted_question)\n",
    "\n",
    "splits = kf.split(attempted_question)\n",
    "\n",
    "run_id = 0\n",
    "total_jacc_score = 0\n",
    "total_questions = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-tours",
   "metadata": {},
   "source": [
    "# Run 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "occupied-nepal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 1675\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = next(splits)\n",
    "\n",
    "train_set = [attempted_question[i] for i in train_index]\n",
    "test_set = [attempted_question[i] for i in test_index]\n",
    "\n",
    "total_examples = 0\n",
    "with open('pos_eg_gt', 'w') as out:\n",
    "    for t in train_set:\n",
    "        examples = parser.get_pos_example(t)\n",
    "        total_examples += len(examples)\n",
    "        for e in examples:\n",
    "            print(e.gen_example(), file=out)\n",
    "print(F'Total examples: {total_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlling-shepherd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tests:     177\n",
      "jacc score:   125.03333333333333\n",
      "norm jacc score: 0.7064030131826742\n",
      "\n",
      "PARSING ERROR\n",
      "[91833]\n",
      "\n",
      "NOT FULL SCORE\n",
      "[85689, 72563, 52058, 87060, 94890, 78852, 71186, 33374, 51101, 63329, 12193, 61212, 3120]\n",
      "\n",
      "ZERO SCORE\n",
      "[59566, 45810, 113521, 107346, 117818, 1260, 18350, 65891, 9143, 60940, 63936, 50348, 38557, 36135, 90819, 13001, 52351, 83808, 48275, 21145, 50385, 121282, 102623, 119619, 45155, 93719, 20082, 76092, 25385, 34678, 112266, 94974, 67705, 45415, 62681, 33258, 1210, 66632, 15575, 41227, 71942, 108251, 37191, 2540, 50755]\n"
     ]
    }
   ],
   "source": [
    "jacc, n_q = one_iter_gt(train_set, test_set)\n",
    "total_jacc_score += jacc\n",
    "total_questions += n_q\n",
    "run_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-vaccine",
   "metadata": {},
   "source": [
    "# Run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acoustic-essex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 1752\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = next(splits)\n",
    "\n",
    "train_set = [attempted_question[i] for i in train_index]\n",
    "test_set = [attempted_question[i] for i in test_index]\n",
    "\n",
    "total_examples = 0\n",
    "with open('pos_eg_gt', 'w') as out:\n",
    "    for t in train_set:\n",
    "        examples = parser.get_pos_example(t)\n",
    "        total_examples += len(examples)\n",
    "        for e in examples:\n",
    "            print(e.gen_example(), file=out)\n",
    "print(F'Total examples: {total_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "effective-triangle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tests:     175\n",
      "jacc score:   128.43333333333334\n",
      "norm jacc score: 0.733904761904762\n",
      "\n",
      "PARSING ERROR\n",
      "[110880, 11144]\n",
      "\n",
      "NOT FULL SCORE\n",
      "[81523, 62904, 45051, 92313, 101817, 68703, 6457, 16543, 81406, 95158, 80467]\n",
      "\n",
      "ZERO SCORE\n",
      "[38543, 117851, 60166, 3969, 11773, 46461, 118476, 52597, 87330, 16559, 63962, 55624, 94366, 104234, 41489, 83645, 107768, 3888, 12408, 86452, 26737, 45011, 11818, 17278, 66631, 79944, 114436, 66736, 19447, 16182, 48221, 19331, 68289, 20562, 111835, 84532, 38737, 28204, 22658, 8575]\n"
     ]
    }
   ],
   "source": [
    "jacc, n_q = one_iter_gt(train_set, test_set)\n",
    "total_jacc_score += jacc\n",
    "total_questions += n_q\n",
    "run_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-adaptation",
   "metadata": {},
   "source": [
    "# Run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acceptable-chocolate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 1751\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = next(splits)\n",
    "\n",
    "train_set = [attempted_question[i] for i in train_index]\n",
    "test_set = [attempted_question[i] for i in test_index]\n",
    "\n",
    "total_examples = 0\n",
    "with open('pos_eg_gt', 'w') as out:\n",
    "    for t in train_set:\n",
    "        examples = parser.get_pos_example(t)\n",
    "        total_examples += len(examples)\n",
    "        for e in examples:\n",
    "            print(e.gen_example(), file=out)\n",
    "print(F'Total examples: {total_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eight-parks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tests:     177\n",
      "jacc score:   124.16666666666667\n",
      "norm jacc score: 0.7015065913370998\n",
      "\n",
      "PARSING ERROR\n",
      "[]\n",
      "\n",
      "NOT FULL SCORE\n",
      "[17424, 97636, 4797, 97719, 122032, 107372, 33418, 28253, 120918, 110401, 6478, 13720, 67921, 75310, 47217]\n",
      "\n",
      "ZERO SCORE\n",
      "[32606, 1842, 28739, 118198, 113145, 12737, 32579, 73159, 79282, 26776, 9808, 97405, 45895, 12822, 60959, 49133, 57218, 119983, 34666, 26986, 7558, 16146, 50524, 79329, 30878, 36166, 27617, 20340, 38381, 48743, 18042, 84903, 42612, 90782, 22192, 66214, 75040, 89083, 34699, 107151, 58701, 81589, 6553, 63602, 115376]\n"
     ]
    }
   ],
   "source": [
    "jacc, n_q = one_iter_gt(train_set, test_set)\n",
    "total_jacc_score += jacc\n",
    "total_questions += n_q\n",
    "run_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-clearing",
   "metadata": {},
   "source": [
    "# Run 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "forty-carpet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 1722\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = next(splits)\n",
    "\n",
    "train_set = [attempted_question[i] for i in train_index]\n",
    "test_set = [attempted_question[i] for i in test_index]\n",
    "\n",
    "total_examples = 0\n",
    "with open('pos_eg_gt', 'w') as out:\n",
    "    for t in train_set:\n",
    "        examples = parser.get_pos_example(t)\n",
    "        total_examples += len(examples)\n",
    "        for e in examples:\n",
    "            print(e.gen_example(), file=out)\n",
    "print(F'Total examples: {total_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "opening-posting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tests:     175\n",
      "jacc score:   124.94999999999999\n",
      "norm jacc score: 0.714\n",
      "\n",
      "PARSING ERROR\n",
      "[5374, 64394]\n",
      "\n",
      "NOT FULL SCORE\n",
      "[94232, 100045, 101233, 119541, 81602, 34859, 47714, 115405, 38768, 62756, 78301, 92516, 10847, 73907, 2179, 101976]\n",
      "\n",
      "ZERO SCORE\n",
      "[54981, 58600, 108741, 48878, 60504, 58347, 42604, 98014, 111420, 41401, 104181, 112558, 623, 90459, 96813, 43717, 35102, 13796, 7398, 16523, 61614, 100757, 115726, 64917, 28343, 10881, 22928, 29251, 1112, 29320, 47096, 4770, 10199, 57838, 91217, 90367, 60429, 111187, 102598, 89700, 16643]\n"
     ]
    }
   ],
   "source": [
    "jacc, n_q = one_iter_gt(train_set, test_set)\n",
    "total_jacc_score += jacc\n",
    "total_questions += n_q\n",
    "run_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-mongolia",
   "metadata": {},
   "source": [
    "# Run 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "falling-portugal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 1760\n"
     ]
    }
   ],
   "source": [
    "train_index, test_index = next(splits)\n",
    "\n",
    "train_set = [attempted_question[i] for i in train_index]\n",
    "test_set = [attempted_question[i] for i in test_index]\n",
    "\n",
    "total_examples = 0\n",
    "with open('pos_eg_gt', 'w') as out:\n",
    "    for t in train_set:\n",
    "        examples = parser.get_pos_example(t)\n",
    "        total_examples += len(examples)\n",
    "        for e in examples:\n",
    "            print(e.gen_example(), file=out)\n",
    "print(F'Total examples: {total_examples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "about-indonesian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# tests:     175\n",
      "jacc score:   125.83333333333333\n",
      "norm jacc score: 0.719047619047619\n",
      "\n",
      "PARSING ERROR\n",
      "[56128, 116486]\n",
      "\n",
      "NOT FULL SCORE\n",
      "[41402, 90225, 117747, 109516]\n",
      "\n",
      "ZERO SCORE\n",
      "[44553, 40734, 54160, 113117, 33554, 105758, 17298, 14773, 93301, 56096, 112862, 37031, 103778, 104027, 96969, 71850, 7962, 83484, 27275, 14598, 95907, 94077, 42450, 677, 6753, 63786, 57942, 276, 25490, 42970, 84175, 14167, 45751, 87754, 114725, 40491, 106967, 32721, 45905, 80819, 19711, 13911, 117755, 121820, 27411, 60915, 114775]\n"
     ]
    }
   ],
   "source": [
    "jacc, n_q = one_iter_gt(train_set, test_set)\n",
    "total_jacc_score += jacc\n",
    "total_questions += n_q\n",
    "run_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "rapid-entertainment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jacc score: 628.4166666666666\n",
      "Total questions:  879\n",
      "Avg norm jacc:    0.7149222601441031\n"
     ]
    }
   ],
   "source": [
    "print(F'Total jacc score: {total_jacc_score}')\n",
    "print(F'Total questions:  {total_questions}')\n",
    "print(F'Avg norm jacc:    {total_jacc_score /  total_questions}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from image_processing import draw_bounding_box\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "test = a_q_dict[25720]\n",
    "\n",
    "# time_span_to_timestamps_list(test)\n",
    "vid_folder = frame_folder + test['vid_name'] + '/'\n",
    "\n",
    "time = 30\n",
    "qa_objects = od.get_frame_qa_objects(vid_folder, 0.7, time)\n",
    "bboxes = [o.bbox for o in qa_objects]\n",
    "display(Image.fromarray(draw_bounding_box(vid_folder + F'000{time}.jpg', bboxes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-excitement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
